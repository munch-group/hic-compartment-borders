{
  "hash": "a8ec47b51b7fcd70351fb05461a5875a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"master_workflow\"\nauthor: Søren Jørgensen\ndate: last-modified\nexecute: \n  enabled: false\n---\n\nfrom gwf import *\nimport os\nimport subprocess\nimport os.path as op\nimport pandas as pd\nfrom cooler.fileops import list_coolers\nfrom pprintpp import pprint as pp\n\n######################################\n\n`gwf` workflow for the full pipeline from downloading raw reads \nto compartmentalization analysis. We are using `bwa mem` for mapping. \n\nHow to run:\nconda activate hic\ngwf -f master_workflow.py status\n\nWorkflow:\n  1a  bwa_index        : Index the reference genome with `bwa index`\n  1b  sam_index        : Index the fasta again with `samtools faidx`\n  1c  download_reads   : Download raw reads from SRA\n                        `sra-downloader` with SRR IDs (paired-end reads)\n  2   bwa_map          : Map reads (PE) to the reference with `bwa mem`\n  3   pair_sort        : Pair the merged alignments from mate-pair sequencing with `pairtools parse`,\n                        then sort with `pairtools sort`\n  4   dedup            : Deduplicate the sorted pairs with `pairtools dedup`\n  5   make_pairs_cool  : Create a .cool file from the deduplicated pairs with `cooler cload pairs`\n\n######################################\n\nCreate a workflow object\ngwf = Workflow(defaults={'nodes': 1, 'queue':\"normal\", 'account':\"hic-spermatogenesis\"})\n\n############################################\n############## Templates ###################\n############################################\n\ndef download_index_reference(name, refdir):\n    \"\"\"\n    Download the reference genome from NCBI, and index it for bwa\n    \"\"\"\n    out_dir = op.join(refdir, name)\n    inputs = []\n    outputs = [f\"{out_dir}/index/bwa/{name}.fa\",\n               f\"{out_dir}/index/bwa/{name}.fa.amb\",\n               f\"{out_dir}/index/bwa/{name}.fa.ann\",\n               f\"{out_dir}/index/bwa/{name}.fa.bwt\",\n               f\"{out_dir}/index/bwa/{name}.fa.pac\",\n               f\"{out_dir}/index/bwa/{name}.fa.sa\",\n               f\"{out_dir}/assembly_report.txt\",\n               f\"{out_dir}/{name}.fa.sizes\",\n               f\"{out_dir}/{name}.fa.fai\",\n               f\"{out_dir}/{name}.gaps.bed\",\n               ]\n    options = {\n        'memory': '16g',\n        'walltime': '01:00:00',\n        'cores': 8,\n    }\n    spec = f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\ngenomepy plugin enable bwa && \\\ngenomepy install {name} --provider ncbi --genomes_dir {refdir} --threads 8\n    \"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\ndef download_reads(sra_downloader, srr_id, read_dir):\n    \"\"\"Download raw reads from SRA\"\"\"\n    inputs  = [sra_downloader]\n    outputs = [f\"{read_dir}/{srr_id}_1.fastq.gz\",\n               f\"{read_dir}/{srr_id}_2.fastq.gz\",\n               f\"{read_dir}/dl_{srr_id}.done\"]\n    options = {'cores':16, 'memory': \"4g\", 'walltime':\"06:00:00\"}\n    spec = f\"\"\"\nsingularity run {sra_downloader} {srr_id} --save-dir {read_dir} --cores 16 && \\\ntouch {read_dir}/dl_{srr_id}.done\n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\ndef bwa_index(ref_genome):\n    \"\"\"Template for indexing the reference genome with bwa\"\"\"\n    inputs  = [ref_genome]\n    outputs = [f\"{ref_genome}.amb\", \n               f\"{ref_genome}.ann\", \n               f\"{ref_genome}.bwt\", \n               f\"{ref_genome}.pac\", \n               f\"{ref_genome}.sa\"]\n    options = {'cores':1, 'memory': \"8g\", 'walltime':\"02:00:00\"}\n    spec = f'''\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\nbwa index -p {ref_genome} -a bwtsw {ref_genome}\n'''\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\ndef sam_index(ref_genome):\n    \"\"\"Creating a fasta index. `bwa mem` also needs a fasta index generated by samtools\"\"\"\n    inputs = [ref_genome]\n    outputs = [f\"{ref_genome}.fai\"]\n    options = {'cores':1, 'memory':\"1g\", 'walltime':\"00:20:00\"}\n    spec=f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\nsamtools faidx {ref_genome}\n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\ndef bwa_map(ref_genome, mate_1, mate_2, download_ok, out_bam):\n    \"\"\"\n    Template for mapping reads to a reference genome using `bwa` and `samtools`. \n    NB! Here, we map the mates together, as bwa states it is no problem for Hi-C reads. \n    \"\"\"\n    threads = 32\n    inputs = [f\"{ref_genome}.amb\", \n              f\"{ref_genome}.ann\", \n              f\"{ref_genome}.bwt\", \n              f\"{ref_genome}.pac\", \n              f\"{ref_genome}.sa\", \n              #f\"{ref_genome}.fai\",\n              mate_1, mate_2,\n              download_ok]\n    outputs = [out_bam]\n    options = {'cores':threads, 'memory': \"32g\", 'walltime':\"24:00:00\"}\n    spec = f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\nbwa mem -t {threads} -SP {ref_genome} {mate_1} {mate_2} > {out_bam}\n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\nUpdated version uses --walks-policy 5unique in stead of mask\ndef pair_sort_alignments(chromsizes, bam_merged, sorted_pairs):\n    \"\"\"Convert the paired-end alignments to .pairs with `pairtools parse`, then sort\"\"\"\n    inputs = [bam_merged]\n    outputs = [f\"{bam_merged}_parsed.stats\", \n               sorted_pairs]\n    options = {'cores':32, 'memory':\"16g\", 'walltime':\"10:00:00\"}\n    spec=f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\npairtools parse \\\n    -c {chromsizes} \\\n    --drop-sam --drop-seq \\\n    --output-stats {bam_merged}_parsed.stats \\\n    --add-columns mapq \\\n    --assembly rheMac10 \\\n    --walks-policy 5unique \\\n    {bam_merged} | \\\npairtools sort -o {sorted_pairs} \n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\ndef dedup(sorted_pairs, chromsizes):\n    \"\"\"Deduplicate the sorted pairs with `pairtools dedup`\"\"\"\n    pairs_prefix = sorted_pairs.split(\".sorted\")[0]\n    inputs = [sorted_pairs]\n    outputs = [f\"{pairs_prefix}.nodups.pairs.gz\",\n               f\"{pairs_prefix}.unmapped.pairs.gz\",\n               f\"{pairs_prefix}.dups.pairs.gz\",\n               f\"{pairs_prefix}.dedup.stats\",\n               f\"{pairs_prefix}.dedup.done\"]\n    options = {'cores':12, 'memory': \"1g\", 'walltime': \"03:30:00\"}\n    spec = f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\npairtools dedup \\\n    --max-mismatch 3 \\\n    --mark-dups \\\n    --chunksize 100000 \\\n    --chrom-subset {chromsizes} \\\n    --output {pairs_prefix}.nodups.pairs.gz \\\n    --output-unmapped {pairs_prefix}.unmapped.pairs.gz \\\n    --output-stats {pairs_prefix}.dedup.stats \\\n    --output-dups {pairs_prefix}.dups.pairs.gz \\\n    {sorted_pairs} && \\\ntouch {pairs_prefix}.dedup.done\n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\nUpdated version uses pairtools select to filter out low quality reads\ndef make_pairs_cool(chromsizes, pairs, cool_out):\n    \"\"\"Create coolers from pairs with `cooler cload pairs`\"\"\"\n    base = os.path.basename(pairs).split(\".pairs.gz\")[0]\n    inputs = [pairs]\n    outputs = [cool_out]\n    options = {'cores':1, 'memory':\"8g\", 'walltime':\"06:30:00\"}\n    spec = f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\npairtools select \"(mapq1>=30) and (mapq2>=30)\" {pairs} | \\\ncooler cload pairs \\\n    -c1 2 -p1 3 -c2 4 -p2 5 \\\n    --assembly 'T2T-MMU8v1' \\\n    --chunksize 200000 \\\n    {chromsizes}:1000 \\\n    - \\\n    {cool_out}\n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n    \ndef merge_zoomify_balance(cooler_list, merged, mcool):\n    \"\"\"Merge a given list of coolers with `cooler merge`\"\"\"\n    cooler_list_unix_str = \" \".join(cooler_list)\n    inputs = [cooler_list]\n    outputs = [merged, mcool]\n    options = {'cores':16, 'memory':\"32g\", 'walltime':\"04:00:00\"}\n    spec = f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\ncooler merge -c 50000000 {merged} {cooler_list_unix_str} && \\\ncooler zoomify --nproc 16 \\\n    --resolutions 1000,5000,10000,50000,100000 \\\n    --balance \\\n    --balance-args '--nproc 16' \\\n    -o {mcool} \\\n    {merged}\n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\nDRAFT: Not implemented as a target yet\ndef balance_cooler_cis(cool_in, cool_out):\n    \"\"\"Balance a cooler with `cooler balance`\"\"\"\n    mcool = cool_in.split(\"::\")[0]\n    inputs = [mcool]\n    outputs = [cool_out]\n    options = {'cores':8, 'memory':\"32g\", 'walltime':\"03:00:00\"}\n    spec = f\"\"\"\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate hic\ncooler balance -p 32 --cis-only --name cis_weights {cool_in} && \\\ntouch {cool_out}\n\"\"\"\n    return AnonymousTarget(inputs=inputs, outputs=outputs, options=options, spec=spec)\n\n\n###############################################\nSet up the folder structure for the workflow #\n###############################################   \n \n # Define our working dir\nmain_dir = op.dirname(__file__)\nmsc_dir = op.join(main_dir, \"../hic-spermatogenesis/\")\n\n\nDefine the reads directory (download reads here)\nreads_dir = op.join(\"steps/macaque_raw/downloaded/\")\n\nCheck to find SRA-downloader, else pull from docker\nsra_downloader = \"steps/sra-downloader.sif\"\n\nif not op.exists(sra_downloader):\n    command = [\"singularity\", \"pull\", sra_downloader, \"docker://wwydmanski/sra-downloader\"]\n    try:\n        result = subprocess.run(command, check=True, capture_output=True, text=True)\n        print(result.stdout)\n        print(result.stderr)\n    except subprocess.CalledProcessError as e:\n        print(e.stdout) \n        print(e.stderr)\n\n\nDefine subdirs for tissue type, strip_whitespace and make lowercase\nsra_runtable = pd.read_csv(\"data/SraRunTable.txt\")\nreads_subdirs = set(sra_runtable[\"source_name\"])\n\nMake a dict mapping the tissue type ['source_name'] to the SRR IDs ['Run']\ntissue_dict = {tissue: sra_runtable[sra_runtable[\"source_name\"]==tissue][\"Run\"].tolist() for tissue in reads_subdirs}\n\nDefine the output dirs for files\nbam_dir = op.join(main_dir,\"steps/bamfiles\") \npair_dir = op.join(main_dir, \"steps/pairs\")  \ncool_dir = op.join(main_dir, \"steps/cool\") \n\n\n############################################\n############## Targets #####################\n############################################\n\nDownload the reference genome\nref_name = \"T2T-MMU8v1.0\"\nref_dir = op.join(main_dir, \"steps/ref\")\n\nT0 = gwf.target_from_template(\n    f\"install_T2T_MMU8V1\", \n    download_index_reference(ref_name, ref_dir)\n    )\n\nref_genome = T0.outputs[0]\nref_dir = op.join(ref_dir, ref_name) ## Update ref_dir\nchromsizes = op.join(ref_dir, f\"{ref_name}.fa.sizes\")\n\n\n# Index the reference genome\nT1a = gwf.target_from_template(f\"bwa_index_{os.path.basename(ref_genome).split('.')[0]}\", \n                               bwa_index(ref_genome=ref_genome))\nT1b = gwf.target_from_template(f\"sam_index_{os.path.basename(ref_genome).split('.')[0]}\", \n                               sam_index(ref_genome=ref_genome))\n\n\nT5out is a dict to store the cool files for each tissue type\nto be merged in the end\nT5out = {}\n\nT1c target for each ID in the SRA runtable\nfor id in sra_runtable[\"Run\"]:\n    \n    # Do some stuff before downloading the reads\n    # Get the tissue type for the ID, and assign a subdirectory for the reads\n    sub_dir = None\n    for cell_type, ids in tissue_dict.items():\n        if id in ids:\n            sub_dir = cell_type.lower().replace(\" \", \"_\")\n            break\n    if sub_dir is None:\n        raise ValueError(f\"ID {id} not found in any tissue type\")\n\n    fastq_dir = op.join(reads_dir, sub_dir)\n\n    # Create subdirs if they don't exist\n    if not op.exists(fastq_dir):\n        os.makedirs(fastq_dir)\n\n    # T1c: Download SRA ID\n    T1c = gwf.target_from_template(f\"dl_{id}\",\n                                  download_reads(\n                                        sra_downloader = sra_downloader,\n                                        srr_id = id,\n                                        read_dir = fastq_dir\n                                    ))\n    \n    \n    # T2: Map the reads to the reference genome\n    bam_wdir = op.join(bam_dir, sub_dir)\n    out_bam = op.join(bam_wdir, f\"{id}.bam\")\n\n    if not op.exists(bam_wdir):\n        os.makedirs(bam_wdir)\n\n    T2 = gwf.target_from_template(f\"bwa_map_{id}\", \n                                  bwa_map(ref_genome=ref_genome, \n                                          mate_1=T1c.outputs[0], mate_2=T1c.outputs[1], \n                                          download_ok=T1c.outputs[2],\n                                          out_bam=out_bam))\n    \n    # T3: pair and sort the alignments\n    pair_subdir = op.join(pair_dir, sub_dir)\n    sorted_pairs = op.join(pair_subdir, f\"{id}.sorted.pairs.gz\")\n\n    if not op.exists(pair_subdir):\n        os.makedirs(pair_subdir)\n\n    T3 = gwf.target_from_template(f\"parse_{id}\",\n                                    pair_sort_alignments(chromsizes=chromsizes, \n                                                         bam_merged=T2.outputs[0], \n                                                         sorted_pairs=sorted_pairs))\n    \n    # T4: Deduplicate the sorted pairs\n    # Dedup dir is the same as pair dir\n    # NB we are using the reduced chromsizes file (only 'real' chromosomes)\n    T4 = gwf.target_from_template(f\"dedup_{id}\",\n                                    dedup(sorted_pairs=T3.outputs[1],\n                                            chromsizes=chromsizes))\n\n    # T5: Create a .cool file from the deduplicated pairs\n    cool_subdir = op.join(cool_dir, sub_dir)\n\n    if not op.exists(cool_subdir):\n        os.makedirs(cool_subdir)\n\n    T4outpairs = [x for x in T4.outputs if x.endswith(\".pairs.gz\")]\n    if sub_dir not in T5out:\n        T5out[sub_dir] = []\n\n\n\n    for T4out in T4outpairs:\n        # Filter to be removed if we want to\n        # Only make a cool file of the 'nodups' pairs\n        if 'nodups' not in T4out:\n            continue\n\n        cool_name = f\"{id}\"\n        cool_file = os.path.join(cool_subdir, f\"{cool_name}.1000.cool\")\n\n        T5 = gwf.target_from_template(f\"coolify_{cool_name}\",\n                                        make_pairs_cool(chromsizes, pairs=T4out, cool_out=cool_file))\n        T5out[sub_dir].append(T5.outputs[0])\n        \n\nMerge the coolers for each tissue type (group)\nT6out = {}\nfor subdir,cool_list in T5out.items():\n    T6out[subdir] = gwf.target_from_template(f\"merge_zoom_balance_{subdir}\",\n                                    merge_zoomify_balance(cooler_list=cool_list, \n                                                          merged=op.join(cool_dir, subdir, f\"{subdir}.merged.cool\"), \n                                                          mcool=op.join(cool_dir, subdir, f\"{subdir}.merged.mcool\")))\npp(T6out[subdir].inputs[0])\n\n",
    "supporting": [
      "workflow_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}